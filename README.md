# sparker
Postgres - https://postgresapp.com/downloads.html
GUI - https://www.pgadmin.org

db\table = task\task

inserting - default user (postgres)

Main class - src\main\scala\Main 

first build - then run

table config - WebService.scala bottom of the file

## Working with AVRO

http://avro.apache.org/docs/current/gettingstartedjava.html

https://spark.apache.org/docs/latest/sql-data-sources-avro.html



###Задача

Часть 1 – модуль загрузки.

Цель – из внешнего источника взять данные и отправить их в Платформу Кибербезопасности (хранилище данных) – а Платформу
  будут моделировать следующие приложения.
  
- [x] Есть файлы csv в локальной файловой системе – event и ext_fact.
Читаем их через Spark.

Валидируем.
 - [x] Парсинг YAML. Обязательность полей, соответствие форматам и т.д. – правила в виде yml
 -файла, указаны не для всех полей, если для какого-то поля не указаны – не валидируем его. Смысл правил расписан в начале yml-файла в комментарии.
 - [x] Если в какой-то строке невалидные значения – вся строка откладывается в отдельный файл error.csv.
 - [x] Джойним (inner join) датафреймы event и ext_fact по полю event_id.
 - [x] Полученный датафрейм фильтруем по двум условиям: type_operation = RurPayment и event_channel = MOBILE.
 - [x] Каждую строку полученного датафрейма (каждую транзакцию) сериализуем в JSON
 - [x] и отправляем на REST API второго модуля (Http-метод POST, в теле JSON).
 - [x] Если не отправляется (connection refused
  или какое-то такое исключение возникает), то ждем какое-то время и пытаемся ещё раз. Структура сообщения – плоский JSON, все поля на одном уровне.
 - [x] Поля, которые должны быть в JSON-е: event_id,event_time,event_channel,sub_channel,event_type,sub_type,
 event_description,transaction_amount,transaction_sender_account_number,transaction_beneficiar_account_number,ccaf_dt_load,event_dt,
 issue_date_card_owner,number_dul,account_number_of_recipient,number_card_recepient,payer_card_number,recepient_bik,
 recepient_inn,recepient_fio,client_phone_number
 - [x] Параметры приложения: путь к файлу event, путь к файлу ext_fact, путь к файлу конфигурации (validate.yml
 ), адрес REST API второго модуля, количество попыток отправки и интервал между ними в секундах.
 - [x] Параметры должны читаться из командной строки.
 - [ ] В результате должны получить исполняемый джарник, который можно запустить командой «java –jar some.jar [key=value
 , key=value, …]»

Часть 2 – сервис (интерфейсный модуль).
- [x] Веб-сервис, висит на порту, слушает входящие POST-запросы. Адрес (endpoint, путь) согласовать с автором загрузки.
 Протокол HTTP без SSL пока что и без аутентификации.
- [x] Очень простая логика, нужно только тело запроса (JSON) сохранить в БД Postgres. Имя схемы и таблицы на усмотрение
  автора (согласовать с автором модуля парсинга). Поля таблицы: id, дата приёма сообщения, статус (0 – сохранено, 1 –
  обработано и загружено в Платформу, 2 – обработано с ошибкой), текст ошибки, тело запроса. Статус при сохранении
  ставим 0, остальные ставит следующий модуль. Текст ошибки при сохранении NULL.
Параметры приложения: хост и порт, на котором будет висеть приложение; данные для подключения к БД, логин/пароль; можно
  также имя схемы/таблицы вынести в параметры, чтобы сервис мог писать в разные таблицы.

Часть 3 – парсинг и сохранение в Платформу.
- [x] Spark-приложение, запускается вручную, или по расписанию, т.е. независимо от сервиса.
- [x] Приложение читает из Postgres новые входящие запросы в статусе 0, парсит их средствами Spark (по приложенной Avro
-схеме – файл schema.json).
- [ ] Если в теле запроса невалидный JSON – обновляем статус на 2 (ошибка), пишем текст ошибки «invalid JSON
» или подобный, на усмотрение. При всех остальных также сохранять по этим строкам статус 2, приложение вообще не должно фатально завершаться из-за ошибок обработки сообщений (хотя вообще и может падать из-за каких-то общих проблем, типа нехватки памяти).
- [x] Сообщения, которые успешно распарсили, сохраняем в Платформу (ну физически пусть это будут Parquet
-файлы) в две таблицы (2 логических слоя):
1.       Src-слой – данные ровно в том виде, как они пришли из внешнего источника. По сути это аналог таблицы в Postgres. Поля src-слоя – ID (можно генерировать рандомный UUID для этого), дата создания строки в формате YYYYMMDD (партиционировать датафрйем по этому полю перед записью в Parquet), и текст исходного сообщения (сам JSON, вот как есть).
2.       Inc-слой – данные в табличном виде, полезном для просмотра, анализа и выполнения SQL. То есть тут должен получиться parquet-файл с полями, имена полей как в JSON-е, типы в соответствии с Avro-схемой.
- [x] Параметры приложения: данные для подключения к БД, логин/пароль; путь к файлу Avro
-схемы, имя схемы/таблицы в Postgres, откуда читать сообщения (можно будет натравливать с разными схемами на разные таблицы);

Общие пожелания ко всем модулям:
- [x] 1.       Текстовый лог писать (библиотеку логирования log4j2
 можно использовать). События и уровни логирования на усмотрения авторов, по логу должно быть понятно человеку со стороны, что происходит (сотруднику группы сопровождения, который мало что знает о бизнес-смысле приложений). Т.е. какие-то логические ошибки и ошибки валидации должны быть видны в логе.
- [ ] 2.       Все джарники должны быть fat jar, т.е. запускаться командой java –jar
 – все зависимости должны быть упакованы туда. Пока так сделаем, для простоты. В результате там, где используется Spark, получатся джарники размером в сотни мегабайт, потому что Spark сам по себе большой – ну и ладно
- [x] 3.       Версия Spark – 2.4.0, версия Scala – 2.11

- [ ] Развитие на будущее:
1.       Серверная SSL-аутентификация между загрузкой и сервисом – автор сервиса выпускает самоподписанный сертификат, настраивает сервис на его использование, загрузка его добавляет в своё хранилище доверенных сертификатов и по нему проверяет подлинность сервиса



общие пожелания:
1. убрать неиспользуемый код (классы, методы) и ненужные комментарии. Комментарии нужны для пояснения смысла существующего кода. Закомментированный код и ошмётки каких-то проверочных классов не нужны, засоряют, мешают восприятию
2. запросы и вообще всю работу с БД обычно выносят в отдельный класс. Сейчас это всё в WebService, а должна быть там только логика приложения, а в отдельном классе - методы, выполняющие селекты и инсерты в БД
3. везде, кроме может быть саааамых очевидных случаев - пишите тип возвращаемого значения метода и тип переменной. Это улучшает читаемость. Ваш код будут дописывать и читать в общем случае другие люди, пожалейте их. val a = 0 - тут можно не писать. А def createSchema(schemaString: String) = new Schema.Parser().parse(schemaString) - тут лучше написать, потому что взглянул на это - и сразу нихрена не понятно, что возвращает parse? Чтобы это узнать, надо навести мышку на метод, или нажать какую-то комбинацию клавиш, да? Но когда ты целый день читаешь чужой код, и его адски много - лучше явно видеть типы. Я сам кстати не всегда следую этому принципу, и это плохо
4. var заменить на val (ещё вижу, остаются var кое-где). Например, в проекте saver можно было бы сделать case class Config и возвращать его инстанс из метода parseArgs, и дальше везде его использоватьы

com.target.http:
1. sql-скрипты вынести в отдельные файлы. Как правило, они нужны для автодеплоя вашего приложения, то есть некие скрипты или джобы Jenkins будут их запускать. Поэтому отдельные файлы где-то должны быть. См. ниже ещё предложение
2. обычно классы формируют в package, а сейчас этого нет в проекте com.target.http
3. зачем println, если и так все логируется через log4j?
4. try/catch заменить на scala.util.Try (ну и вообще разобраться, как scala.util.Try работает, овладеть им)

Если не знаете еще, что такое Jenkins - почитайте. Пишут джобы сборки обычно devops-инженеры у нас отдельные, но кодеры должны понимать этот процесс, чтобы свои скрипты и файлы в проекте делать эффективно, чтобы при собрке и деплое не было проблем. Это важно. Плохой кодер пишет просто код, который у него в IDE работает. Хороший кодер производит продукт, который эффективно и автоматизированно деплоится на все стенды от дева до прода.

Ещё почитайте, что такое Liquibase. В идеале было бы неплохо, чтобы в проекте com.target.http появился Liquibase changelog, готовый к деплою. Ну попробуйте хотя бы.

saver:
1. sys.exit(1) бросилось в глаза - не видел такого в промышленных проектах. Обычно делают просто throw исключения. Это гарантирует, что прошла вся обработка этого исключения, во всех методах в мтеке вызовов. И в конце концов дефолтный обработчик исключений выведет в stdout весь stacktrace, и это хорошо и полезно для сопровождения, которые потом в логах на проде увидят это и поймут, что произошло. А с sys.exit ты в коде можешь забыть вывести в консоль или в лог, и потом никто ничего не поймёт. Надо всегда думать об удобстве сопровождения твоего продукта
2. LazyLogging - почему одинаковый код в двух проектах? Кто-то у кого-то списал, да? :) надо было договориться и вынести в общий модуль такой общий код, и на него поставить зависимость всех проектов, где это используется. Аналогично ErrorHandler выглядит похожим по сути. То есть я бы все это вынес в под-проект в sparker, и погуглил бы, как в build.sbt прописать зависимости, чтобы при сборке каждого проекта компилировался бы и подключался этот общий 
3. в конструкция типа else logger.info("Dataframe schema successfully validated") - было бы неплохо всегда 
   добавлять скобки {}, это полезная привычка, не отнимает много времени, наверно все IDE умеют при вводе открывающей сразу добавлять и закрывающую. И потом это пригодится в ситуации, когда вы быстро-быстро правите какой-то код, и по запарке после
else
  logger.info("Dataframe schema successfully validated")
добавляете какую-то строку и думаете на автомате, что она выполнится в else, а она всегда выполняется - готов баг. Скобки - явное ограничение
4. в одних запросах таблица подставляется из параметров, а в других захардкожена task - думаю, надо выбрать что-то одно
5. в методе com.target.saver.Saver.getDataframeFromDatabase есть value.count() - как вы может быть читали, у DataFrame есть методы-действия, так вот count - действие, он запускает вычисление датафрейма. В общем случае я бы избегал его использовать для логирования. Ведь до его вызова вы можете применить к датафрему множество методов, т.е. запланировать множество вычислений, сложных и долгих. Даафрейм это грубо говоря план запроса, как вот в БД. Вызовами методов типа withColumn или select вы добавляете в план указания, что следует сделать с данными, но фихически ничего не делается, пока не вызовется один из методов-действий - count, show, write и т.д. В данном случае датафрейм вычисляется путем запроса к БД. Вы выполнении метода .count будет выполнен запрос к БД один раз. Дальше с датафреймом что-то делаете, добаляете в план запроса что-то. Потом выполняете действие toLocalIterator - в этот момент запрос в БД выполняется второй раз. Во-первых, это неэффективно, зарос может быть долгий. Во-вторых, что хуже, между запросами состояние БД может измениться, status поменяется в каких-то строках, и в логе получите одно количество строк, а обработается совсем другое.
6. метод toLocalIterator - я сам ни разу его не использовал, почитал его javadoc - выглядит стрёмно :) "this results in multiple Spark jobs" и т.д. - как будто он тоже несколько раз может пересчитывать датафрейм, то есть обращаться в БД несколько раз
7. тип (String, String, String, DataFrame, Int) выглядит нечитаемо, непонятно, что там внутри. Лучше такие вещи делать в виде case class
8. Петя, ты когда-то сказал, что "забиваешь гвозди микроскопом" - так и есть :) getDFList, validateRecords и saveDfToParquet надо заменить на user defined function, почитайте про них. В реальной жизни они очень полезны и повсеместно используются. То есть логика должна быть примерно такая: зачитали датафрейм из БД, в каждой строке - некий JSON, столбец с ним называется req_body; применили UDF к столбцу req_body, она вернула флаг в новое поле датафрейма - верный/неверный JSON (под "верным" понимать просто совпадения названий полей, можно не сверять типы данных, как сейчас делается в DataFrameSchemaChecker, то есть внутри распарсить JSON той же библиотекой cirсe например, из него выдрать имена полей и сравнить их с именами полей в проверочной схеме, как-то так можно сделать); из полученного датафрейма хорошие строки по этому полю-флагу сохранили в виде результирующих файлов, а плохие... дальше есть варианты, для простоты можно через .collect собрать их на драйвер в локальную колллекцию и по ней обновить статус "ошибка" в БД, либо через метод .mapPartitions делать это в распределённой манере - тут надо помнить, что каждый partition датафрейма в общем случае обрабатывается на разных экзекьюторах (на разных хостах кластера, в разных JVM), поэтому соединения к БД нужно там же и создавать. Это сложный путь. Можно забить и сделать через .collect. А сложности может будете делать, если приживётесь в Сбере и конкретно в нашем управлении на ПКБ.

Не помню, советовал или нет - по Spark есть книга "Изучаем Spark. Молниеносный анализ данных", на обложке какая-то рыба. Почитайте её, там вроде немного, и полезно.

loader:
1. в CirceParser есть warnings, например "match may not be exhaustive. It would fail on the following input: None". Я не знаю, что там в c.focus может быть, но warning подозрителен всегда.
2. в ConfigParser зачем нужен метод fixYaml, не понял? Кстати, если бы использовали Jackson или json4s, все время их путаю, но это те библиотеки, которые в Spark используются - там был бы доступен парсинг прямо в case class через аннотации, ну это необязательно, просто может чуть удобнее
3. SparkSession в приложении обычно создаётся одна, а тут вижу в Loader и DataframeValidator разные, это зачем? В методе getOrCreate конечно есть какое-то кэширование сессий, и может он вернёт одну и ту же, я хз. А может и нет, если перед этим в config указать разную конфигурацию. Так что проще явно один раз создавать SparkSession
4. в com.target.loader.DataframeValidator.validateFacts и com.target.loader.DataframeValidator.validateEvents опять было бы круто использовать UDF, а не собирать collect все строки на драйвер. Вообще, collect надо использовать осторожно. Распределенная обработка больших данных зачем нужна? Затем, чтобы обработать огромную порцию данных, например может быть 100 файлов общим размером 50 Гб - их невозможно обработать в одной JVM, нужен кластер, т.к. файлы тупо не влезут в память одной JVM. И вот если вы на таком огромном датафрейме вызовите .collect - все 50 Гб с экзекьюторов перешлются на драйвер и он рухнет. Так что collect можно делать, только если явно по бизнес-логик приложения понятно, что объём данных датафрейма не велик. А тут у нас что? Исходный файлы содержат некие транзакции, которых может быть очень много, так что стараемся всю обработку делать в распределённой манере
5. в case class ValidateConfig вместо "if (map.containsKey("match")) m = map.get("match")" - можно как-то более в стиле Scala написать getOrDefault("match", ""). А вообще, в других местах, если нужно, чтобы применить магию Scala к коллекциям Java (LinkedHashMap и т.д.), можно использовать полезные классы scala.collection.JavaConversions или scala.collection.JavaConverters, не помню точно, какой актуальный, но в них прописаны implicit-преобразования между типами коллекций Java и Scala, туда-сюда, так что можно просто применять Scala-методы к коллекциям Java, иногда полезно бывает, посмотрите-почитайте про эти классы
6. увидел метод com.target.loader.DataframeValidator.validate, который заджойнил 2 фрейма, убрал пару каких-то полей, и его результат методом com.target.loader.Sender.send уже отправляется в REST API. А где выборка нужных полей? Мне казалось, мы там хотели их множества полей выбрать только несколько, вот которые прописаны в com.target.loader.Globals.columnsForJson - но эта переменная нигде не используется (только в классе com.target.loader.Encoder.com.target.loader.Encoder, который сам не используется). Что-то тут не доделано как будто
7. HttpOptions.readTimeout(10000) в Send - HTTP-таймауты обычно делают параметрами приложения, чтобы если кто-то написал медленный REST API, можно было подрегулировать на проде параметр приложения. Вообще, параметры в Сбере - это круто, если вы узнаете, как трудно тут деплоить обновления на прод - вы захотите все важные значения держать в параметрах. Захардкодили HTTP-таймаут, интегрируетесь с медленным REST API - получаете ошибку, инцидент, и сделат ничего не можете - за 5 минут исправляете в коде таймаут, но чтобы задеплоить на прод новую версию приложения, нужно подождать недельку. Это чисто практический совет для Сбера. Все важное - в параметры! (в молодёжном стартапе деплоите на проде за 5 минут, там можно всё хардкодить вообще, но Сбер так дисциплинирует)
